# üéµ ElevenLabs Audio Animation Implementation Guide

*Generated: September 28, 2025*

## üìã Problem Statement

**Current Situation:**
- ‚úÖ **Working Animation**: VoiceReactiveVisual.tsx works perfectly with FastAPI base64 audio
- ‚ùå **Interview Challenge**: Need to adapt it to work with ElevenLabs URLs instead of base64
- üéØ **Goal**: Keep the exact working animation logic, just change the audio source

**Why Direct ElevenLabs URL Fails:**
```typescript
// ‚ùå This will fail due to CORS:
const audioElement = new Audio('https://api.elevenlabs.io/v1/audio/xyz123');
const source = audioContext.createMediaElementSource(audioElement); // CORS ERROR!
```

---

## üöÄ APPROACH 1: Convert ElevenLabs URL ‚Üí Base64

### **Theory:**
Convert ElevenLabs URL to the exact same base64 format that currently works, then use your existing code unchanged.

### **How It Works:**
```
ElevenLabs URL ‚Üí fetch() ‚Üí arrayBuffer() ‚Üí base64 encoding ‚Üí data:audio/mpeg;base64,... ‚Üí Your existing code
```

### **Implementation:**

#### **Step 1: Create URL to Base64 Converter**
```typescript
const convertUrlToBase64 = async (audioUrl: string): Promise<string> => {
  try {
    // Fetch the audio file
    const response = await fetch(audioUrl);
    if (!response.ok) {
      throw new Error(`Failed to fetch audio: ${response.status}`);
    }
    
    // Convert to array buffer
    const arrayBuffer = await response.arrayBuffer();
    
    // Convert to base64
    const bytes = new Uint8Array(arrayBuffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    const base64 = btoa(binary);
    
    // Return in the exact format your working code expects
    return `data:audio/mpeg;base64,${base64}`;
  } catch (error) {
    console.error('Failed to convert URL to base64:', error);
    throw error;
  }
};
```

#### **Step 2: Modify Props Interface**
```typescript
interface VoiceReactiveVisualProps {
  elevenLabsAudio?: string | null;  // ElevenLabs URL
  isAgentSpeaking: boolean;
  onAgentAudioEnd?: () => void;
  mediaStream?: MediaStream | null; // For mic muting
  className?: string;
}
```

#### **Step 3: Add useEffect for Props Changes**
```typescript
useEffect(() => {
  if (elevenLabsAudio && isAgentSpeaking) {
    handleElevenLabsAudio(elevenLabsAudio);
  }
}, [elevenLabsAudio, isAgentSpeaking]);

const handleElevenLabsAudio = async (audioUrl: string) => {
  try {
    setIsPlaying(true);
    
    // Convert ElevenLabs URL to base64
    const base64Audio = await convertUrlToBase64(audioUrl);
    
    // Use your EXACT existing playAudioAndAnimate function
    await playAudioAndAnimate(base64Audio);
  } catch (error) {
    console.error('Failed to handle ElevenLabs audio:', error);
    setError('Failed to play audio response');
    setIsPlaying(false);
  }
};
```

#### **Step 4: Keep Existing playAudioAndAnimate Unchanged**
```typescript
// This function stays EXACTLY the same as your working version
const playAudioAndAnimate = async (audioBase64: string) => {
  // Your existing working code - NO CHANGES NEEDED
  const audioElement = new Audio(audioBase64); // Now receives base64 from ElevenLabs
  const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
  const source = audioContext.createMediaElementSource(audioElement);
  const analyser = new Analyser(source);
  source.connect(audioContext.destination);
  
  // ... rest of your working animation logic unchanged
};
```

### **Pros:**
- ‚úÖ Zero changes to existing animation logic
- ‚úÖ Identical to current working approach
- ‚úÖ Easy to understand and debug

### **Cons:**
- ‚ùå 33% larger memory usage (base64 overhead)
- ‚ùå Extra encoding step (performance impact)
- ‚ùå Potential memory issues with large audio files

---

## üöÄ APPROACH 2: Fetch + Blob URL (RECOMMENDED)

### **Theory:**
Download ElevenLabs audio as blob, create local blob URL, then use with your existing audio analysis pipeline.

### **How It Works:**
```
ElevenLabs URL ‚Üí fetch() ‚Üí blob() ‚Üí createObjectURL() ‚Üí blob://localhost/... ‚Üí Your existing pipeline
```

### **Implementation:**

#### **Step 1: Modify playAudioAndAnimate Function**
```typescript
const playAudioAndAnimate = async (elevenLabsUrl: string) => { // URL instead of base64
  try {
    setIsPlaying(true);
    
    // NEW: Convert ElevenLabs URL to local blob
    console.log('üéµ Fetching ElevenLabs audio:', elevenLabsUrl.substring(0, 50) + '...');
    
    const response = await fetch(elevenLabsUrl);
    if (!response.ok) {
      throw new Error(`Failed to fetch audio: ${response.status}`);
    }
    
    const audioBlob = await response.blob();
    const blobUrl = URL.createObjectURL(audioBlob);
    
    console.log('‚úÖ Audio blob created:', blobUrl);
    
    // SAME: Your exact working audio setup (unchanged)
    const audioElement = new Audio(blobUrl); // blob URL instead of base64 data URL
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    
    // Resume audio context if suspended (required for autoplay policies)
    if (audioContext.state === 'suspended') {
      await audioContext.resume();
    }
    
    const source = audioContext.createMediaElementSource(audioElement);
    const analyser = new Analyser(source);
    source.connect(audioContext.destination);
    
    // SAME: Store references for animation (unchanged)
    sceneRef.current.audioElement = audioElement;
    sceneRef.current.analyser = analyser;
    
    // SAME: Audio event handlers (with blob cleanup)
    audioElement.onended = () => {
      console.log('üéµ Audio playback finished');
      setIsPlaying(false);
      sceneRef.current.analyser = undefined;
      sceneRef.current.audioElement = undefined;
      audioContext.close();
      URL.revokeObjectURL(blobUrl); // Clean up blob URL
      
      // Callback to parent component
      if (onAgentAudioEnd) {
        onAgentAudioEnd();
      }
    };
    
    audioElement.onerror = (error) => {
      console.error('Audio playback error:', error);
      setError('Failed to play audio response');
      setIsPlaying(false);
      URL.revokeObjectURL(blobUrl); // Clean up on error
    };
    
    // SAME: Play the audio (unchanged)
    await audioElement.play();
    console.log('üéµ Audio playback started');
    
  } catch (err) {
    console.error('Audio setup error:', err);
    setError('Failed to play audio response');
    setIsPlaying(false);
  }
};
```

#### **Step 2: Add Props Interface**
```typescript
interface VoiceReactiveVisualProps {
  elevenLabsAudio?: string | null;  // ElevenLabs URL
  isAgentSpeaking: boolean;
  onAgentAudioEnd?: () => void;
  mediaStream?: MediaStream | null; // For mic muting
  className?: string;
}

export default function VoiceReactiveVisual({ 
  elevenLabsAudio,
  isAgentSpeaking,
  onAgentAudioEnd,
  mediaStream,
  className = '' 
}: VoiceReactiveVisualProps) {
```

#### **Step 3: Add useEffect for Props Changes**
```typescript
useEffect(() => {
  console.log("üé® VoiceReactiveVisual props changed:", {
    elevenLabsAudio: elevenLabsAudio?.substring(0, 50) + '...',
    isAgentSpeaking,
  });
  
  // Clean up previous audio when new audio comes in
  if (sceneRef.current.audioElement) {
    sceneRef.current.audioElement.pause();
    sceneRef.current.audioElement = undefined;
    sceneRef.current.analyser = undefined;
  }
  
  if (elevenLabsAudio && isAgentSpeaking) {
    console.log("üéµ Starting ElevenLabs audio playback and animation...");
    playAudioAndAnimate(elevenLabsAudio);
  } else {
    console.log("üîá Stopping audio playback");
    setIsPlaying(false);
  }
}, [elevenLabsAudio, isAgentSpeaking]);
```

#### **Step 4: Add Microphone Muting (Optional)**
```typescript
// Optional: Add mic muting during agent speech
const muteUserMicrophone = () => {
  if (mediaStream) {
    const audioTrack = mediaStream.getAudioTracks()[0];
    if (audioTrack && audioTrack.enabled) {
      console.log("üîá Muting user microphone during agent speech");
      audioTrack.enabled = false;
    }
  }
};

const unmuteUserMicrophone = () => {
  if (mediaStream) {
    const audioTrack = mediaStream.getAudioTracks()[0];
    if (audioTrack && !audioTrack.enabled) {
      console.log("üîä Unmuting user microphone after agent speech");
      audioTrack.enabled = true;
    }
  }
};

// Call these in useEffect:
useEffect(() => {
  if (elevenLabsAudio && isAgentSpeaking) {
    muteUserMicrophone();
    playAudioAndAnimate(elevenLabsAudio);
  } else {
    unmuteUserMicrophone();
    setIsPlaying(false);
  }
}, [elevenLabsAudio, isAgentSpeaking, mediaStream]);
```

#### **Step 5: Remove FastAPI Dependencies**
```typescript
// REMOVE these imports:
// import { apiService, ApiError } from '../services/api';

// REMOVE these state variables:
// const [inputText, setInputText] = useState('');
// const [error, setError] = useState<string>('');

// REMOVE the handleSendMessage function

// REMOVE all UI components (input field, button, etc.)
```

#### **Step 6: Update Animation Loop (Keep Unchanged)**
```typescript
// Your existing animation loop stays EXACTLY the same
const animate = () => {
  if (!sceneRef.current.sphere || !sceneRef.current.backdrop || !sceneRef.current.composer) {
    sceneRef.current.animationId = requestAnimationFrame(animate);
    return;
  }

  // SAME: All your existing animation logic unchanged
  if (sceneRef.current.analyser && sceneRef.current.audioElement && !sceneRef.current.audioElement.paused) {
    sceneRef.current.analyser.update();
    // ... your existing sphere animation logic
  }
  
  // SAME: Rest of animation loop unchanged
};
```

### **Pros:**
- ‚úÖ More memory efficient than base64
- ‚úÖ Better performance (no encoding overhead)
- ‚úÖ Modern standard approach
- ‚úÖ Explicit memory cleanup
- ‚úÖ Same Web Audio API pipeline as working version

### **Cons:**
- ‚ö†Ô∏è Requires explicit cleanup (`URL.revokeObjectURL()`)
- ‚ö†Ô∏è Slightly more complex than approach 1

---

## üéØ TESTING STRATEGY

### **Step 1: Basic Audio Test**
```typescript
// Test if ElevenLabs audio can be fetched and played
const testElevenLabsAudio = async (url: string) => {
  try {
    const response = await fetch(url);
    const blob = await response.blob();
    const blobUrl = URL.createObjectURL(blob);
    const audio = new Audio(blobUrl);
    await audio.play();
    console.log('‚úÖ ElevenLabs audio playback works');
    URL.revokeObjectURL(blobUrl);
  } catch (error) {
    console.error('‚ùå ElevenLabs audio test failed:', error);
  }
};
```

### **Step 2: Web Audio API Test**
```typescript
// Test if Web Audio API works with ElevenLabs blob
const testWebAudioAPI = async (url: string) => {
  try {
    const response = await fetch(url);
    const blob = await response.blob();
    const blobUrl = URL.createObjectURL(blob);
    const audio = new Audio(blobUrl);
    const audioContext = new AudioContext();
    const source = audioContext.createMediaElementSource(audio); // This should work
    console.log('‚úÖ Web Audio API works with ElevenLabs blob');
    URL.revokeObjectURL(blobUrl);
    audioContext.close();
  } catch (error) {
    console.error('‚ùå Web Audio API test failed:', error);
  }
};
```

### **Step 3: Animation Test**
```typescript
// Test if animation responds to ElevenLabs audio
// Look for console logs:
// "üéµ Audio playback started"
// "üìä Analyser created successfully"
// Watch for sphere movement during audio playback
```

---

## üö® TROUBLESHOOTING GUIDE

### **Common Issues:**

#### **1. CORS Errors**
```
Error: Failed to fetch audio: 403
Solution: Check ElevenLabs API headers, ensure valid API key
```

#### **2. Web Audio API Errors**
```
DOMException: The request is not allowed by the user agent
Solution: Ensure user interaction before creating AudioContext
```

#### **3. Audio Context Suspended**
```
AudioContext in 'suspended' state
Solution: Call audioContext.resume() before playing
```

#### **4. Memory Leaks**
```
Too many blob URLs created
Solution: Always call URL.revokeObjectURL() when done
```

### **Debug Checklist:**

- [ ] ElevenLabs URL is valid and accessible
- [ ] Fetch request succeeds (check network tab)
- [ ] Blob creation succeeds
- [ ] Audio element creation succeeds
- [ ] AudioContext creation succeeds
- [ ] MediaElementSource creation succeeds
- [ ] Audio playback starts
- [ ] Analyser receives frequency data
- [ ] Sphere animation responds to audio

---

## üìù IMPLEMENTATION PRIORITY

**Recommended Order:**

1. **Start with Approach 2 (Fetch + Blob)** - More efficient and modern
2. **If Approach 2 fails**, fallback to Approach 1 (Base64 conversion)
3. **Test thoroughly** with actual ElevenLabs URLs
4. **Add error handling** for network issues
5. **Add cleanup logic** to prevent memory leaks

---

## üéØ SUCCESS CRITERIA

**The implementation is successful when:**

- ‚úÖ ElevenLabs audio plays correctly
- ‚úÖ Sphere animates in sync with audio
- ‚úÖ No CORS errors in console
- ‚úÖ No memory leaks (blob URLs cleaned up)
- ‚úÖ Animation quality matches your working FastAPI version
- ‚úÖ Props integration works with interview flow

---

*This guide provides both implementation approaches. Start with Approach 2 (Fetch + Blob) for optimal performance, with Approach 1 (Base64) as a fallback if needed.*